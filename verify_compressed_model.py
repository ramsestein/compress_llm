#!/usr/bin/env python3
"""
Script para verificar modelos comprimidos guardados por componentes
"""
import os
import sys
import torch
from pathlib import Path
import json
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig
import argparse
import numpy as np
from typing import Dict, Any
import gc
import time

def load_model_from_components(model_dir: Path, device: str = "cpu"):
    """Load a model that was saved using component-based saving."""
    
    # Check if this was saved by components
    metadata_path = model_dir / 'component_save_metadata.json'
    if not metadata_path.exists():
        raise ValueError("Este directorio no contiene un modelo guardado por componentes")
    
    with open(metadata_path, 'r') as f:
        metadata = json.load(f)
    
    print(f"üì¶ Cargando modelo guardado por componentes: {metadata['model_type']}")
    print(f"üìä Total de par√°metros: {metadata['total_parameters']}")
    
    # Load config
    config_path = model_dir / 'config.json'
    if not config_path.exists():
        raise FileNotFoundError("No se encontr√≥ config.json")
    
    try:
        config = AutoConfig.from_pretrained(str(model_dir))
        print("‚úÖ Configuraci√≥n cargada")
    except Exception as e:
        print(f"‚ö†Ô∏è Error cargando config: {e}")
        # Try to create a basic config
        config = AutoConfig.from_pretrained("microsoft/DialoGPT-small")
        print("‚úÖ Configuraci√≥n b√°sica creada")
    
    # Create model from config
    try:
        model = AutoModelForCausalLM.from_config(config)
        print("‚úÖ Modelo creado desde configuraci√≥n")
    except Exception as e:
        print(f"‚ùå Error creando modelo: {e}")
        raise
    
    # Load weights from individual parameter files
    state_dict = {}
    param_files = list(model_dir.glob("*.pt"))
    print(f"üîç Encontrados {len(param_files)} archivos de par√°metros")
    
    for param_file in param_files:
        try:
            # Extract parameter name from filename
            param_name = param_file.stem.replace('_', '.')
            
            # Load parameter
            param_tensor = torch.load(param_file, map_location=device)
            
            # Add to state dict
            state_dict[param_name] = param_tensor
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error cargando par√°metro {param_file.name}: {e}")
            continue
    
    print(f"‚úÖ {len(state_dict)} par√°metros cargados")
    
    # Load state dict into model
    try:
        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
        if missing_keys:
            print(f"‚ö†Ô∏è Claves faltantes: {len(missing_keys)}")
        if unexpected_keys:
            print(f"‚ö†Ô∏è Claves inesperadas: {len(unexpected_keys)}")
        
        print("‚úÖ Estado del modelo cargado exitosamente")
        
    except Exception as e:
        print(f"‚ùå Error cargando estado del modelo: {e}")
        raise
    
    # Move model to device
    model.to(device)
    model.eval()
    
    print(f"‚úÖ Modelo cargado exitosamente en {device}")
    return model

def get_model_info(model_path: Path) -> Dict[str, Any]:
    """Obtiene informaci√≥n detallada de un modelo"""
    info = {
        'path': str(model_path),
        'exists': model_path.exists()
    }
    
    if not info['exists']:
        return info
    
    try:
        # Cargar config
        config_path = model_path / "config.json"
        if config_path.exists():
            with open(config_path, 'r') as f:
                config = json.load(f)
            info['architecture'] = config.get('model_type', 'unknown')
            info['hidden_size'] = config.get('hidden_size', 0)
            info['num_layers'] = config.get('num_hidden_layers', 0)
        
        # Tama√±o en disco
        total_size = 0
        for file_path in model_path.rglob('*'):
            if file_path.is_file():
                total_size += file_path.stat().st_size
        info['disk_size_mb'] = total_size / (1024 * 1024)
        
        # Contar archivos del modelo
        model_files = list(model_path.glob('*.safetensors')) + \
                     list(model_path.glob('*.bin')) + \
                     list(model_path.glob('*.pt'))
        info['model_files'] = len(model_files)
        
        # Verificar metadata de compresi√≥n
        metadata_path = model_path / "compression_metadata.json"
        if metadata_path.exists():
            with open(metadata_path, 'r') as f:
                info['compression_metadata'] = json.load(f)
        
        return info
        
    except Exception as e:
        info['error'] = str(e)
        return info

def compare_outputs(original_model_path: Path, compressed_model_path: Path, prompt: str = "Hello, how are you?"):
    """Compara las salidas de dos modelos con el mismo prompt"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    print(f"\nüî¨ Comparando outputs...")
    print(f"üìù Prompt: '{prompt}'")
    print(f"üñ•Ô∏è Device: {device}")
    
    outputs = {}
    
    # Cargar modelo original
    print(f"\n{'‚îÄ'*50}")
    print(f"Cargando modelo original: {original_model_path.name}")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(str(original_model_path))
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        if device.type == "cuda":
            original_model = AutoModelForCausalLM.from_pretrained(
                str(original_model_path),
                torch_dtype=torch.float16,
                device_map="auto",
                low_cpu_mem_usage=True
            )
        else:
            original_model = AutoModelForCausalLM.from_pretrained(
                str(original_model_path),
                torch_dtype=torch.float32,
                low_cpu_mem_usage=True
            )
            original_model = original_model.to(device)
        
        original_model.eval()
        
        # Generar texto
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        with torch.no_grad():
            start_time = time.time()
            outputs_original = original_model.generate(
                inputs,
                max_length=len(inputs['input_ids'][0]) + 50,
                num_return_sequences=1,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
            generation_time = time.time() - start_time
        
        output_text = tokenizer.decode(outputs_original[0], skip_special_tokens=True)
        outputs['original'] = {
            'text': output_text,
            'time': generation_time,
            'length': len(outputs_original[0])
        }
        
        print(f"‚úÖ Output original: {output_text}")
        print(f"‚è±Ô∏è Tiempo de generaci√≥n: {generation_time:.2f}s")
        
        # Limpiar memoria
        del original_model
        gc.collect()
        
    except Exception as e:
        print(f"‚ùå Error con modelo original: {e}")
        outputs['original'] = {'error': str(e)}
    
    # Cargar modelo comprimido
    print(f"\n{'‚îÄ'*50}")
    print(f"Cargando modelo comprimido: {compressed_model_path.name}")
    
    try:
        compressed_model = load_model_from_components(compressed_model_path, str(device))
        
        # Generar texto
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        with torch.no_grad():
            start_time = time.time()
            outputs_compressed = compressed_model.generate(
                inputs,
                max_length=len(inputs['input_ids'][0]) + 50,
                num_return_sequences=1,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
            generation_time = time.time() - start_time
        
        output_text = tokenizer.decode(outputs_compressed[0], skip_special_tokens=True)
        outputs['compressed'] = {
            'text': output_text,
            'time': generation_time,
            'length': len(outputs_compressed[0])
        }
        
        print(f"‚úÖ Output comprimido: {output_text}")
        print(f"‚è±Ô∏è Tiempo de generaci√≥n: {generation_time:.2f}s")
        
        # Limpiar memoria
        del compressed_model
        gc.collect()
        
    except Exception as e:
        print(f"‚ùå Error con modelo comprimido: {e}")
        outputs['compressed'] = {'error': str(e)}
    
    return outputs

def main():
    parser = argparse.ArgumentParser(
        description='Verifica modelos comprimidos guardados por componentes',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ejemplos:
  # Verificar modelo comprimido
  python verify_compressed_model.py microsoft_DialoGPT-small
  
  # Especificar directorio de modelos
  python verify_compressed_model.py microsoft_DialoGPT-small --models-dir ./models
        """
    )
    
    parser.add_argument(
        'model',
        type=str,
        help='Nombre del modelo a verificar'
    )
    
    parser.add_argument(
        '--models-dir',
        type=str,
        default='./models',
        help='Directorio de modelos (default: ./models)'
    )
    
    parser.add_argument(
        '--prompt',
        type=str,
        default='Hello, how are you?',
        help='Prompt para comparar outputs (default: "Hello, how are you?")'
    )
    
    args = parser.parse_args()
    
    # Construir rutas
    models_dir = Path(args.models_dir)
    original_path = models_dir / args.model
    compressed_path = models_dir / f"{args.model}_compressed"
    
    print("="*60)
    print("üîç VERIFICACI√ìN DE MODELO COMPRIMIDO")
    print("="*60)
    
    # Verificar que existen ambos modelos
    if not original_path.exists():
        print(f"‚ùå No se encontr√≥ el modelo original: {original_path}")
        sys.exit(1)
    
    if not compressed_path.exists():
        print(f"‚ùå No se encontr√≥ el modelo comprimido: {compressed_path}")
        sys.exit(1)
    
    # Analizar informaci√≥n de ambos modelos
    print("\nüìä Analizando modelos...")
    
    original_info = get_model_info(original_path)
    compressed_info = get_model_info(compressed_path)
    
    print(f"\nüì¶ MODELO ORIGINAL: {args.model}")
    print(f"{'‚îÄ'*50}")
    print(f"‚úÖ Ubicaci√≥n: {original_info['path']}")
    print(f"   Arquitectura: {original_info.get('architecture', 'unknown')}")
    print(f"   Tama√±o en disco: {original_info.get('disk_size_mb', 0):.1f} MB")
    print(f"   Archivos del modelo: {original_info.get('model_files', 0)}")
    
    print(f"\nüì¶ MODELO COMPRIMIDO: {args.model}_compressed")
    print(f"{'‚îÄ'*50}")
    print(f"‚úÖ Ubicaci√≥n: {compressed_info['path']}")
    print(f"   Arquitectura: {compressed_info.get('architecture', 'unknown')}")
    print(f"   Tama√±o en disco: {compressed_info.get('disk_size_mb', 0):.1f} MB")
    print(f"   Archivos del modelo: {compressed_info.get('model_files', 0)}")
    
    # Calcular estad√≠sticas de compresi√≥n
    original_size = original_info.get('disk_size_mb', 0)
    compressed_size = compressed_info.get('disk_size_mb', 0)
    
    if original_size > 0 and compressed_size > 0:
        reduction = ((original_size - compressed_size) / original_size) * 100
        factor = original_size / compressed_size
        
        print(f"\nüìä Estad√≠sticas de compresi√≥n:")
        print(f"   üìâ Reducci√≥n lograda:")
        print(f"      Tama√±o original: {original_size:.1f} MB")
        print(f"      Tama√±o final: {compressed_size:.1f} MB")
        print(f"      Reducci√≥n: {reduction:.1f}%")
        print(f"      Factor: {factor:.1f}x m√°s peque√±o")
    
    # Comparar outputs
    outputs = compare_outputs(original_path, compressed_path, args.prompt)
    
    # Mostrar resumen
    print(f"\n{'‚îÄ'*50}")
    print("üìù RESUMEN DE COMPARACI√ìN")
    print(f"{'‚îÄ'*50}")
    
    if 'original' in outputs and 'compressed' in outputs:
        if 'error' not in outputs['original'] and 'error' not in outputs['compressed']:
            print("‚úÖ Ambos modelos generaron texto exitosamente")
            
            # Comparar tiempos
            original_time = outputs['original']['time']
            compressed_time = outputs['compressed']['time']
            time_ratio = compressed_time / original_time if original_time > 0 else 0
            
            print(f"‚è±Ô∏è Tiempo original: {original_time:.2f}s")
            print(f"‚è±Ô∏è Tiempo comprimido: {compressed_time:.2f}s")
            print(f"üìä Ratio de tiempo: {time_ratio:.2f}x")
            
            if time_ratio > 1.5:
                print("‚ö†Ô∏è El modelo comprimido es significativamente m√°s lento")
            elif time_ratio < 0.8:
                print("üéâ El modelo comprimido es m√°s r√°pido!")
            else:
                print("‚úÖ Rendimiento similar")
        else:
            print("‚ùå Error en la generaci√≥n de texto")
            if 'error' in outputs['original']:
                print(f"   Modelo original: {outputs['original']['error']}")
            if 'error' in outputs['compressed']:
                print(f"   Modelo comprimido: {outputs['compressed']['error']}")
    
    print(f"\n‚úÖ Verificaci√≥n completada!")
    print(f"üí° Pr√≥ximos pasos:")
    print(f"   1. Fine-tuning: python finetune_lora.py --model {args.model}_compressed")
    print(f"   2. Servidor Ollama: python ollama_compact_server.py --model {args.model}_compressed")

if __name__ == "__main__":
    main()
