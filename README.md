# ğŸš€ Compress LLM - Sistema de CompresiÃ³n y Fine-tuning de Modelos de Lenguaje

Un sistema completo para comprimir y mejorar modelos de lenguaje usando tÃ©cnicas avanzadas de compresiÃ³n y fine-tuning.

## ğŸ“‹ Â¿QuÃ© hace este proyecto?

Este proyecto te permite:

- **Comprimir modelos grandes** para que ocupen menos espacio y sean mÃ¡s rÃ¡pidos
- **Mejorar modelos** con tÃ©cnicas de fine-tuning como LoRA, IA3, BitFit, etc.
- **Optimizar el uso de memoria** en tu computadora
- **Crear modelos personalizados** para tareas especÃ­ficas

## ğŸ› ï¸ Requisitos del Sistema

### MÃ­nimos:
- **Windows 10/11** o **Linux** o **macOS**
- **Python 3.8** o superior
- **8 GB de RAM** (16 GB recomendado)
- **10 GB de espacio libre** en disco

### Recomendados:
- **GPU NVIDIA** con 8+ GB de VRAM (para entrenamiento mÃ¡s rÃ¡pido)
- **32 GB de RAM** (para modelos grandes)
- **100 GB de espacio libre** (para mÃºltiples modelos)

## ğŸ“¦ InstalaciÃ³n Paso a Paso

### 1. Descargar el Proyecto

```bash
# OpciÃ³n 1: Clonar desde Git
git clone https://github.com/tu-usuario/compress_llm_git.git
cd compress_llm_git

# OpciÃ³n 2: Descargar ZIP
# Descarga el archivo ZIP desde GitHub y extrÃ¡elo
```

### 2. Instalar Dependencias

Abre una terminal (Command Prompt en Windows) en la carpeta del proyecto y ejecuta:

```bash
# Instalar todas las dependencias automÃ¡ticamente
python install.py
```

**O manualmente:**

```bash
# Crear entorno virtual
python -m venv comp_venv

# Activar entorno virtual
# En Windows:
comp_venv\Scripts\activate
# En Linux/Mac:
source comp_venv/bin/activate

# Instalar dependencias
pip install -r requirements.txt
```

## ğŸ¯ CÃ³mo Usar el Sistema

### OpciÃ³n 1: Interfaz Interactiva (Recomendado para principiantes)

```bash
# Ejecutar el asistente interactivo
python finetune_peft.py
```

Esto te guiarÃ¡ paso a paso para:
- Seleccionar tu modelo
- Elegir el mÃ©todo de fine-tuning
- Configurar parÃ¡metros
- Iniciar el entrenamiento

### OpciÃ³n 2: Scripts EspecÃ­ficos

#### Para Comprimir un Modelo:

```bash
# Comprimir un modelo existente
python apply_compression.py --model_path "ruta/al/modelo" --output_path "modelo_comprimido"
```

#### Para Fine-tuning con LoRA:

```bash
# Entrenar con LoRA
python finetune_lora.py
```

#### Para Verificar un Modelo:

```bash
# Verificar que un modelo funciona correctamente
python test_compressed_model.py --model_path "ruta/al/modelo"
```

## ğŸ”§ MÃ©todos Disponibles

### MÃ©todos de CompresiÃ³n:
- **INT8/INT4/INT2**: Reduce la precisiÃ³n de los nÃºmeros
- **Pruning**: Elimina conexiones innecesarias
- **SVD/Tucker/MPO**: Descompone matrices grandes
- **Mixed Precision**: Usa diferentes precisiones segÃºn necesidad

### MÃ©todos de Fine-tuning:
- **LoRA**: Mejora eficiente con matrices de bajo rango
- **IA3**: AdaptaciÃ³n rÃ¡pida con pocos parÃ¡metros
- **BitFit**: Solo entrena los sesgos
- **Adapter**: Inserta capas adaptativas
- **QLoRA**: LoRA con cuantizaciÃ³n
- **DoRA**: LoRA con descomposiciÃ³n de magnitud
- **MoLoRA**: LoRA con mÃºltiples expertos
- **Compacter/KronA/S4/Houlsby**: MÃ©todos avanzados de adaptaciÃ³n

## ğŸ“ Estructura del Proyecto

```
compress_llm_git/
â”œâ”€â”€ ğŸ“ create_compress/          # Sistema de compresiÃ³n
â”œâ”€â”€ ğŸ“ LoRa_train/              # Sistema de fine-tuning
â”œâ”€â”€ ğŸ“ datasets/                # Datos de entrenamiento
â”œâ”€â”€ ğŸ“ models/                  # Modelos guardados
â”œâ”€â”€ ğŸ“ finetuned_models/        # Modelos entrenados
â”œâ”€â”€ ğŸ“ compression_analysis/    # Reportes de compresiÃ³n
â”œâ”€â”€ ğŸ“ tests/                   # Tests del sistema
â”œâ”€â”€ ğŸ finetune_peft.py        # Interfaz principal
â”œâ”€â”€ ğŸ apply_compression.py    # Script de compresiÃ³n
â”œâ”€â”€ ğŸ finetune_lora.py        # Script de LoRA
â””â”€â”€ ğŸ“„ requirements.txt         # Dependencias
```

## ğŸš€ Ejemplos de Uso

### Ejemplo 1: Comprimir un Modelo

```bash
# 1. Descargar un modelo (ejemplo con DialoGPT)
python -c "from transformers import AutoModelForCausalLM; AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-small').save_pretrained('./models/mi_modelo')"

# 2. Comprimir el modelo
python apply_compression.py --model_path "./models/mi_modelo" --output_path "./models/mi_modelo_comprimido"
```

### Ejemplo 2: Fine-tuning con LoRA

```bash
# 1. Preparar datos de entrenamiento
# Coloca tus archivos .csv en la carpeta datasets/

# 2. Ejecutar fine-tuning
python finetune_peft.py
# Sigue las instrucciones en pantalla
```

### Ejemplo 3: Servidor Ollama

```bash
# Crear un servidor compatible con Ollama
python ollama_compact_server.py --models_dir "./models"
```

## âš ï¸ SoluciÃ³n de Problemas

### Error: "No module named 'torch'"
```bash
pip install torch torchvision torchaudio
```

### Error: "CUDA out of memory"
- Reduce el tamaÃ±o del batch
- Usa un modelo mÃ¡s pequeÃ±o
- Cierra otras aplicaciones que usen GPU

### Error: "Not enough disk space"
- Libera espacio en disco
- Usa compresiÃ³n mÃ¡s agresiva
- Elimina modelos temporales

### Error: "Python not found"
- AsegÃºrate de que Python estÃ© en el PATH
- Reinstala Python marcando "Add to PATH"

## ğŸ“Š Monitoreo y Reportes

El sistema genera automÃ¡ticamente:

- **Reportes de compresiÃ³n**: AnÃ¡lisis detallado de la compresiÃ³n
- **MÃ©tricas de entrenamiento**: PÃ©rdida, precisiÃ³n, etc.
- **Uso de memoria**: EstadÃ­sticas de RAM y VRAM
- **Tiempos de ejecuciÃ³n**: DuraciÃ³n de cada proceso

Los reportes se guardan en:
- `compression_analysis/` para anÃ¡lisis de compresiÃ³n
- `finetuned_models/` para resultados de entrenamiento

## ğŸ”’ Seguridad y Privacidad

- **Datos locales**: Todo se procesa en tu computadora
- **Sin conexiÃ³n**: No se envÃ­an datos a servidores externos
- **Modelos propios**: Puedes usar tus propios modelos y datos

## ğŸ¤ Contribuir

Para contribuir al proyecto:

1. Haz un fork del repositorio
2. Crea una rama para tu feature
3. Haz commit de tus cambios
4. Abre un Pull Request

## ğŸ“ Soporte

Si tienes problemas:

1. **Revisa esta documentaciÃ³n**
2. **Ejecuta los tests**: `python tests/run_all_tests.py`
3. **Busca en Issues** de GitHub
4. **Crea un nuevo Issue** con detalles del problema

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la licencia MIT. Ver `LICENSE` para mÃ¡s detalles.

## ğŸ™ Agradecimientos

- Hugging Face por las librerÃ­as de transformers
- Microsoft por DialoGPT
- La comunidad de open source

---

**Â¡Disfruta comprimiendo y mejorando tus modelos de lenguaje! ğŸ‰**
