#!/usr/bin/env python3
"""
Script principal para analizar modelos locales o de HuggingFace
"""
import os
import sys
import argparse
from typing import List, Optional, Union
from pathlib import Path
import shutil
import torch
from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForCausalLM
from huggingface_hub import snapshot_download
from tqdm import tqdm
import json
from datetime import datetime

# Importar nuestros m√≥dulos
from down_report.model_analyzer import OptimizedModelAnalyzer
from down_report.report_generator import OptimizedReportGenerator

class ModelManager:
    """Gestiona modelos locales y remotos"""
    
    def __init__(self, models_dir: str = "./models"):
        self.models_dir = Path(models_dir)
        self.models_dir.mkdir(parents=True, exist_ok=True)
    
    def resolve_model_path(self, model_identifier: str) -> tuple[str, bool]:
        """
        Resuelve si un identificador es un modelo local o remoto
        
        Returns:
            tuple: (ruta_del_modelo, es_local)
        """
        # Caso 1: Es una ruta absoluta o relativa existente
        path = Path(model_identifier)
        if path.exists() and (path / "config.json").exists():
            return str(path.resolve()), True
        
        # Caso 2: Es un nombre de modelo en el directorio models/
        safe_name = model_identifier.replace('/', '_')
        local_path = self.models_dir / safe_name
        if local_path.exists() and (local_path / "config.json").exists():
            return str(local_path.resolve()), True
        
        # Caso 3: Buscar por coincidencia parcial en models/
        if not '/' in model_identifier:  # Si no tiene formato org/model
            for model_dir in self.models_dir.iterdir():
                if model_identifier.lower() in model_dir.name.lower():
                    if (model_dir / "config.json").exists():
                        print(f"üîç Encontrado modelo local: {model_dir.name}")
                        return str(model_dir.resolve()), True
        
        # Caso 4: Es un modelo de HuggingFace
        return model_identifier, False
    
    def get_model_info(self, model_path: str) -> dict:
        """Obtiene informaci√≥n de un modelo local"""
        path = Path(model_path)
        
        if not path.exists():
            return None
        
        try:
            config = AutoConfig.from_pretrained(str(path))
            
            # Calcular tama√±o
            total_size = sum(
                os.path.getsize(os.path.join(root, file))
                for root, _, files in os.walk(path)
                for file in files
            )
            
            # Contar archivos de modelo
            model_files = list(path.glob("*.bin")) + list(path.glob("*.safetensors"))
            
            info = {
                'name': path.name,
                'path': str(path),
                'size_gb': total_size / (1024**3),
                'architecture': getattr(config, 'model_type', 'unknown'),
                'hidden_size': getattr(config, 'hidden_size', None),
                'num_layers': getattr(config, 'num_hidden_layers', None),
                'num_parameters': getattr(config, 'num_parameters', None),
                'model_files': len(model_files),
                'total_files': sum(1 for _ in path.rglob('*') if _.is_file())
            }
            
            # Informaci√≥n adicional si existe
            if (path / "download_info.txt").exists():
                info['has_download_info'] = True
            
            return info
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error leyendo informaci√≥n del modelo: {e}")
            return None
    
    def list_local_models(self, detailed: bool = False) -> List[dict]:
        """Lista todos los modelos locales disponibles"""
        models = []
        
        # Buscar en el directorio de modelos
        for model_dir in self.models_dir.iterdir():
            if model_dir.is_dir():
                info = self.get_model_info(str(model_dir))
                if info:
                    models.append(info)
        
        # Ordenar por nombre
        models.sort(key=lambda x: x['name'])
        
        if detailed:
            print(f"\nüìö Modelos locales en {self.models_dir}:")
            print("=" * 80)
            
            if not models:
                print("No se encontraron modelos locales")
                print(f"\nüí° Descarga modelos con: python {sys.argv[0]} <modelo> --download-only")
            else:
                total_size = 0
                for i, model in enumerate(models, 1):
                    print(f"\n{i}. {model['name']}")
                    print(f"   üìÅ Ruta: {model['path']}")
                    print(f"   üèóÔ∏è Arquitectura: {model['architecture']}")
                    print(f"   üíæ Tama√±o: {model['size_gb']:.2f} GB")
                    if model['num_layers']:
                        print(f"   üî¢ Capas: {model['num_layers']}")
                    print(f"   üìÑ Archivos: {model['model_files']} modelo, {model['total_files']} total")
                    total_size += model['size_gb']
                
                print(f"\nüìä Total: {len(models)} modelos, {total_size:.2f} GB")
        
        return models
    
    def download_model(self, model_name: str, force: bool = False) -> tuple[str, dict]:
        """Descarga un modelo de HuggingFace"""
        safe_name = model_name.replace('/', '_')
        local_path = self.models_dir / safe_name
        
        # Verificar si ya existe
        if local_path.exists() and not force:
            print(f"‚úÖ El modelo ya existe en: {local_path}")
            print(f"   (usa --force-download para re-descargar)")
            return str(local_path), self.get_model_info(str(local_path))
        
        print(f"\nüì• Descargando: {model_name}")
        print(f"üìÅ Destino: {local_path}")
        
        try:
            # Descargar usando snapshot_download
            print("‚è≥ Descargando archivos...")
            
            downloaded_path = snapshot_download(
                repo_id=model_name,
                local_dir=str(local_path),
                local_dir_use_symlinks=False,
                resume_download=True,
                tqdm_class=tqdm
            )
            
            # Guardar informaci√≥n de descarga
            info_file = local_path / "download_info.txt"
            with open(info_file, 'w') as f:
                f.write(f"Modelo: {model_name}\n")
                f.write(f"Fecha descarga: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"Fuente: HuggingFace Hub\n")
            
            # Obtener informaci√≥n
            model_info = self.get_model_info(str(local_path))
            
            print(f"\n‚úÖ Descarga completada!")
            print(f"üìä Tama√±o: {model_info['size_gb']:.2f} GB")
            print(f"üèóÔ∏è Arquitectura: {model_info['architecture']}")
            
            return str(local_path), model_info
            
        except Exception as e:
            print(f"\n‚ùå Error descargando modelo: {str(e)}")
            # Limpiar descarga parcial
            if local_path.exists():
                shutil.rmtree(local_path)
            raise


def main():
    parser = argparse.ArgumentParser(
        description='Analiza modelos de lenguaje para optimizaci√≥n y compresi√≥n',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ejemplos:
  # Analizar modelo local
  python analyze_model.py ./models/llama-7b
  
  # Descargar y analizar modelo de HuggingFace
  python analyze_model.py meta-llama/Llama-2-7b-hf
  
  # Solo descargar sin analizar
  python analyze_model.py mistralai/Mistral-7B-v0.1 --download-only
  
  # Listar modelos locales
  python analyze_model.py --list
  
  # An√°lisis para caso de uso espec√≠fico
  python analyze_model.py modelo --use-case rag --calibration-file texts.txt
        """
    )
    
    # Argumento principal
    parser.add_argument(
        'model',
        type=str,
        nargs='?',
        help='Modelo a analizar (HuggingFace ID, ruta local, o nombre parcial)'
    )
    
    # Opciones de gesti√≥n de modelos
    parser.add_argument(
        '--download-only',
        action='store_true',
        help='Solo descargar el modelo sin analizar'
    )
    parser.add_argument(
        '--list', '-l',
        action='store_true',
        help='Listar modelos locales disponibles'
    )
    parser.add_argument(
        '--models-dir',
        type=str,
        default='./models',
        help='Directorio para modelos (default: ./models)'
    )
    parser.add_argument(
        '--force-download',
        action='store_true',
        help='Forzar re-descarga aunque exista'
    )
    
    # Opciones de an√°lisis
    parser.add_argument(
        '--output-dir', '-o',
        type=str,
        default='./compression_analysis',
        help='Directorio para resultados (default: ./compression_analysis)'
    )
    parser.add_argument(
        '--calibration-file', '-c',
        type=str,
        help='Archivo con textos de calibraci√≥n'
    )
    parser.add_argument(
        '--device', '-d',
        type=str,
        default='cuda',
        choices=['cuda', 'cpu'],
        help='Dispositivo para an√°lisis (default: cuda)'
    )
    parser.add_argument(
        '--use-case', '-u',
        type=str,
        choices=['rag', 'ner', 'chatbot', 'agent', 'all'],
        default='all',
        help='Caso de uso espec√≠fico para optimizar (default: all)'
    )
    
    args = parser.parse_args()
    
    # Crear gestor de modelos
    manager = ModelManager(args.models_dir)
    
    # Opci√≥n: listar modelos
    if args.list:
        manager.list_local_models(detailed=True)
        return
    
    # Verificar que se proporcion√≥ un modelo
    if not args.model:
        # Modo interactivo si no se especifica modelo
        models = manager.list_local_models(detailed=True)
        
        if models:
            print("\n¬øQu√© modelo quieres analizar?")
            print("Ingresa el n√∫mero o el nombre del modelo (o 'q' para salir): ")
            
            choice = input("\n> ").strip()
            
            if choice.lower() == 'q':
                return
            
            try:
                # Intentar como n√∫mero
                idx = int(choice) - 1
                if 0 <= idx < len(models):
                    args.model = models[idx]['path']
                else:
                    print("‚ùå N√∫mero inv√°lido")
                    return
            except ValueError:
                # Usar como nombre
                args.model = choice
        else:
            parser.error("No se encontraron modelos locales. Especifica un modelo para descargar.")
    
    # Resolver modelo (local o remoto)
    model_path, is_local = manager.resolve_model_path(args.model)
    
    # Si es remoto y se pidi√≥ download-only
    if not is_local and args.download_only:
        try:
            local_path, info = manager.download_model(model_path, args.force_download)
            print(f"\nüí° Para analizar este modelo, usa:")
            print(f"   python {sys.argv[0]} {local_path}")
        except Exception as e:
            print(f"‚ùå Error: {str(e)}")
            sys.exit(1)
        return
    
    # Si es remoto y no se pidi√≥ download-only, preguntar
    if not is_local:
        print(f"\nüåê Modelo remoto detectado: {model_path}")
        print("¬øQu√© deseas hacer?")
        print("1. Descargar y analizar")
        print("2. Solo descargar")
        print("3. Cancelar")
        
        choice = input("\nOpci√≥n (1/2/3): ").strip()
        
        if choice == '2':
            try:
                local_path, info = manager.download_model(model_path, args.force_download)
                print(f"\nüí° Para analizar este modelo, usa:")
                print(f"   python {sys.argv[0]} {local_path}")
            except Exception as e:
                print(f"‚ùå Error: {str(e)}")
                sys.exit(1)
            return
        elif choice == '3':
            return
        elif choice == '1':
            # Descargar primero
            try:
                print("\nüì• Descargando modelo antes de analizar...")
                model_path, info = manager.download_model(model_path, args.force_download)
                is_local = True
            except Exception as e:
                print(f"‚ùå Error descargando: {str(e)}")
                sys.exit(1)
        else:
            print("‚ùå Opci√≥n inv√°lida")
            return
    
    # Mostrar informaci√≥n del modelo
    if is_local:
        info = manager.get_model_info(model_path)
        if info:
            print(f"\nüìã Informaci√≥n del modelo:")
            print(f"   üìÅ Ruta: {info['path']}")
            print(f"   üèóÔ∏è Arquitectura: {info['architecture']}")
            print(f"   üíæ Tama√±o: {info['size_gb']:.2f} GB")
            if info['num_layers']:
                print(f"   üî¢ Capas: {info['num_layers']}")
    
    # Preparar an√°lisis
    os.makedirs(args.output_dir, exist_ok=True)
    
    # Cargar textos de calibraci√≥n
    calibration_texts = None
    if args.calibration_file and os.path.exists(args.calibration_file):
        with open(args.calibration_file, 'r') as f:
            calibration_texts = [line.strip() for line in f if line.strip()]
        print(f"\nüìÑ Cargados {len(calibration_texts)} textos de calibraci√≥n")
    
    # Ejecutar an√°lisis
    try:
        print(f"\nüîç Iniciando an√°lisis...")
        
        # Cargar el modelo primero
        print("üì• Cargando modelo en memoria...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16 if args.device == 'cuda' else torch.float32,
            device_map='auto' if args.device == 'cuda' else None,
            low_cpu_mem_usage=True
        )
        
        # Mover a dispositivo si es necesario
        if args.device == 'cpu' and hasattr(model, 'to'):
            model = model.to('cpu')
        
        # Obtener estad√≠sticas del modelo
        model_stats = {
            'model_path': model_path,
            'device': args.device,
            'num_layers': len([n for n, _ in model.named_modules() if 'layer' in n]),
            'architecture': model.config.model_type if hasattr(model, 'config') else 'unknown'
        }
        
        # Crear analizador con el modelo cargado
        analyzer = OptimizedModelAnalyzer(model, model_stats)
        
        # Analizar
        strategies = analyzer.analyze(calibration_texts, quick_mode=True, use_case=args.use_case)
        
        # Generar reporte
        model_name_safe = Path(model_path).name if is_local else args.model.replace('/', '_')
        
        # Crear generador de reportes
        report_generator = OptimizedReportGenerator(args.output_dir)
        
        # Preparar datos para el reporte
        analysis_data = {
            'model_info': {
                'name': model_name_safe,
                'analysis_date': datetime.now().isoformat(),
                'path': model_path
            },
            'model_stats': model_stats,
            'compression_recommendations': strategies,
            'layer_summary': _get_layer_summary(analyzer.layer_profiles),
            'complete_layer_analysis': {
                'all_layers': {name: vars(profile) for name, profile in analyzer.layer_profiles.items()}
            }
        }
        
        # Generar reporte HTML
        report_path = report_generator.generate_analysis_report(
            model_name_safe,
            analysis_data,
            format='html'
        )
        
        json_report_path = report_generator.generate_analysis_report(
            model_name_safe,
            analysis_data,
            format='json'
        )
        
        complete_report_path = args.output_dir + "/" + f"{model_name_safe}_report_complete.json"
        with open(complete_report_path, 'w') as f:
            json.dump(analysis_data, f, indent=2, default=str)
        print(f"üìÑ Reporte JSON completo guardado en: {complete_report_path}")

        # Mostrar resumen
        print(_generate_summary(strategies, analyzer.model_stats))
        
        # Si se especific√≥ un caso de uso, mostrar detalles
        if args.use_case != 'all':
            if args.use_case in strategies:
                strategy = strategies[args.use_case]
                print(f"\nüéØ Optimizaci√≥n para {args.use_case.upper()}:")
                print(f"   Compresi√≥n total: {strategy.get('expected_compression', 0)*100:.1f}%")
                print(f"   Tama√±o final: {strategy.get('final_size_mb', 0):.1f} MB")
                print(f"   Performance esperado: {strategy.get('expected_performance', 0)*100:.0f}%")
            else:
                print(f"\n‚ö†Ô∏è Estrategia '{args.use_case}' no disponible.")
                print(f"   Estrategias disponibles: {', '.join(strategies.keys())}")
        
        print(f"\n‚úÖ An√°lisis completado!")
        print(f"üìä Informe guardado en: {report_path}")

        # Liberar memoria del modelo
        del model
        torch.cuda.empty_cache() if args.device == 'cuda' else None
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è An√°lisis interrumpido por el usuario")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Error durante el an√°lisis: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

def _get_layer_summary(layer_profiles):
    """Genera resumen de capas por tipo"""
    summary = {}
    total_size = 0
    
    for name, profile in layer_profiles.items():
        layer_type = profile.type
        if layer_type not in summary:
            summary[layer_type] = {
                'count': 0,
                'total_size_mb': 0,
                'layers': []
            }
        
        summary[layer_type]['count'] += 1
        summary[layer_type]['total_size_mb'] += profile.size_mb
        total_size += profile.size_mb
    
    # Calcular porcentajes
    for layer_type, info in summary.items():
        info['percentage_of_model'] = (info['total_size_mb'] / total_size * 100) if total_size > 0 else 0
        info['average_size_mb'] = info['total_size_mb'] / info['count'] if info['count'] > 0 else 0
    
    return summary

def _generate_summary(strategies, model_stats):
    """Genera resumen en texto"""
    lines = [
        "\n" + "="*80,
        "üìä RESUMEN DE AN√ÅLISIS",
        "="*80,
        f"Tama√±o del modelo: {model_stats.get('total_size_mb', 0):.1f} MB",
        f"Estrategia recomendada: {strategies.get('recommended', 'balanced')}",
        ""
    ]
    
    # Mostrar opciones de compresi√≥n
    for strategy_name in ['conservative', 'balanced', 'aggressive']:
        if strategy_name in strategies:
            strategy = strategies[strategy_name]
            compression = strategy.get('estimated_compression', 0)
            final_size = model_stats.get('total_size_mb', 0) * (1 - compression)
            lines.append(
                f"‚Ä¢ {strategy_name.capitalize()}: "
                f"{compression*100:.0f}% compresi√≥n ‚Üí {final_size:.1f} MB"
            )
    
    lines.append("="*80)
    return '\n'.join(lines)

if __name__ == "__main__":
    main()