#!/usr/bin/env python3
"""
Script para testear qu√© m√≥dulos est√°n disponibles en el modelo comprimido
"""
import torch
from pathlib import Path
import json
from transformers import GPT2Config

def test_model_modules():
    """Test simple para ver qu√© m√≥dulos est√°n disponibles"""
    
    model_dir = Path("models/microsoft_DialoGPT-small_compressed")
    
    if not model_dir.exists():
        print("‚ùå No se encontr√≥ el modelo comprimido")
        return
    
    print("üß™ Testeando m√≥dulos del modelo comprimido...")
    
    try:
        # Cargar configuraci√≥n
        config_path = model_dir / 'config.json'
        with open(config_path, 'r') as f:
            config_dict = json.load(f)
        
        # Convertir diccionario a objeto de configuraci√≥n
        config = GPT2Config(**config_dict)
        
        # Crear modelo desde configuraci√≥n
        from transformers import AutoModelForCausalLM
        model = AutoModelForCausalLM.from_config(config)
        
        print("‚úÖ Modelo creado desde configuraci√≥n")
        
        # Cargar par√°metros guardados por componentes
        param_files = list(model_dir.glob('*.pt'))
        if not param_files:
            print("‚ùå No se encontraron archivos de par√°metros .pt")
            return
        
        print(f"üîç Encontrados {len(param_files)} archivos de par√°metros")
        
        # Crear state_dict
        state_dict = {}
        for param_file in param_files:
            param_name = param_file.stem
            param_data = torch.load(param_file, map_location='cpu')
            state_dict[param_name] = param_data
        
        # Cargar par√°metros en el modelo
        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
        
        print(f"üìä Par√°metros cargados: {len(state_dict)}")
        print(f"‚ö†Ô∏è Claves faltantes: {len(missing_keys)}")
        print(f"‚ö†Ô∏è Claves inesperadas: {len(unexpected_keys)}")
        
        # Listar todos los m√≥dulos disponibles
        print("\nüîç M√≥dulos disponibles en el modelo:")
        all_modules = []
        for name, module in model.named_modules():
            all_modules.append(name)
            if len(all_modules) <= 20:  # Solo mostrar los primeros 20
                print(f"  - {name}: {type(module).__name__}")
        
        if len(all_modules) > 20:
            print(f"  ... y {len(all_modules) - 20} m√°s")
        
        # Buscar m√≥dulos espec√≠ficos para LoRA
        print("\nüéØ M√≥dulos candidatos para LoRA:")
        lora_candidates = []
        for name in all_modules:
            if any(keyword in name for keyword in ['attn.c_attn', 'attn.c_proj', 'mlp.c_fc', 'mlp.c_proj']):
                lora_candidates.append(name)
                print(f"  ‚úÖ {name}")
        
        if not lora_candidates:
            print("  ‚ùå No se encontraron m√≥dulos candidatos para LoRA")
            print("  üîç Buscando m√≥dulos alternativos...")
            
            # Buscar m√≥dulos que contengan 'weight' en el state_dict
            weight_modules = []
            for name in state_dict.keys():
                if 'weight' in name and not name.endswith('_bias'):
                    weight_modules.append(name)
                    if len(weight_modules) <= 10:
                        print(f"    üìù {name}")
            
            if len(weight_modules) > 10:
                print(f"    ... y {len(weight_modules) - 10} m√°s")
        
        print("\n‚úÖ Test completado!")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_model_modules()
