#!/usr/bin/env python3
"""
Script principal para aplicar compresi√≥n a modelos seg√∫n configuraci√≥n JSON
Versi√≥n con soporte completo para todos los m√©todos de compresi√≥n
"""
import os
import sys
import json
import argparse
import shutil
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
import torch
import torch.nn as nn
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    AutoConfig,
    PreTrainedModel
)
from tqdm import tqdm
import logging
from datetime import datetime
import gc

# Importar el motor de compresi√≥n
from create_compress.compression_engine import CompressionEngine

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ModelCompressor:
    """Gestor principal de compresi√≥n de modelos"""
    
    def __init__(self, compression_config_path: str, models_dir: str = "./models", 
                 output_suffix: str = "_compressed"):
        self.config_path = Path(compression_config_path)
        self.models_dir = Path(models_dir)
        self.output_suffix = output_suffix
        
        # Cargar configuraci√≥n
        self.compression_config = self._load_compression_config()
        self.model_name = self.compression_config['metadata']['model_name']
        self.model_path = self.models_dir / self.model_name
        self.output_path = self.models_dir / f"{self.model_name}{self.output_suffix}"
        
        # Motor de compresi√≥n
        self.engine = CompressionEngine()
        
        # Estad√≠sticas
        self.stats = {
            'original_size_mb': 0,
            'compressed_size_mb': 0,
            'layers_compressed': 0,
            'layers_preserved': 0,
            'compression_time_seconds': 0,
            'final_layers_compressed': 0,
            'methods_used': set()
        }
    
    def _load_compression_config(self) -> Dict[str, Any]:
        """Carga la configuraci√≥n de compresi√≥n"""
        if not self.config_path.exists():
            raise FileNotFoundError(f"No se encontr√≥ configuraci√≥n: {self.config_path}")
        
        with open(self.config_path, 'r') as f:
            config = json.load(f)
        
        logger.info(f"‚úÖ Configuraci√≥n cargada: {self.config_path.name}")
        logger.info(f"üì¶ Modelo objetivo: {config['metadata']['model_name']}")
        logger.info(f"üéØ Perfil: {config['global_settings']['profile']}")
        logger.info(f"üìä Compresi√≥n objetivo: {config['global_settings']['target_compression']*100:.1f}%")
        
        # Verificar si hay configuraci√≥n de capas finales
        if config.get('final_layers_config'):
            final_count = config.get('final_layers_count', 0)
            logger.info(f"üéØ Configuraci√≥n especial para las √∫ltimas {final_count} capas")
        
        return config
    
    def _get_layer_config(self, layer_name: str, layer_type: str, 
                         relative_position: float, layer_index: int, 
                         total_layers: int) -> Dict[str, Any]:
        """Obtiene configuraci√≥n de compresi√≥n para una capa espec√≠fica"""
        
        # Verificar si es una capa final con configuraci√≥n especial
        final_layers_config = self.compression_config.get('final_layers_config')
        final_layers_count = self.compression_config.get('final_layers_count', 0)
        
        if final_layers_config and final_layers_count > 0:
            # Calcular si esta capa est√° en las capas finales
            layers_from_end = total_layers - layer_index
            if layers_from_end <= final_layers_count:
                logger.debug(f"üéØ Aplicando configuraci√≥n de capas finales a: {layer_name} (capa {layers_from_end} desde el final)")
                self.stats['final_layers_compressed'] += 1
                return final_layers_config
        
        # Verificar si la capa est√° en la lista de preservadas
        preserved_layers = self.compression_config.get('preserved_layers', [])
        if layer_name in preserved_layers:
            logger.info(f"üõ°Ô∏è Preservando capa sin cambios: {layer_name}")
            self.stats['layers_preserved'] += 1
            return {
                'methods': [{'name': 'none', 'strength': 0.0}],
                'total_compression_ratio': 0.0
            }
        
        # Usar configuraci√≥n por tipo de capa
        layer_configs = self.compression_config.get('layer_configs', {})
        if layer_type in layer_configs:
            return layer_configs[layer_type]
        
        # Default: sin compresi√≥n
        logger.warning(f"‚ö†Ô∏è No hay configuraci√≥n para tipo '{layer_type}', preservando: {layer_name}")
        return {
            'methods': [{'name': 'none', 'strength': 0.0}],
            'total_compression_ratio': 0.0
        }
    
    def compress_model(self):
        """Ejecuta la compresi√≥n del modelo"""
        start_time = datetime.now()
        
        # Verificar que el modelo existe
        if not self.model_path.exists():
            raise FileNotFoundError(f"No se encontr√≥ el modelo: {self.model_path}")
        
        logger.info(f"\n{'='*60}")
        logger.info(f"üöÄ INICIANDO COMPRESI√ìN DE MODELO")
        logger.info(f"{'='*60}")
        logger.info(f"üì¶ Modelo: {self.model_name}")
        logger.info(f"üìÅ Entrada: {self.model_path}")
        logger.info(f"üìÅ Salida: {self.output_path}")
        logger.info(f"{'='*60}\n")
        
        # Crear directorio de salida
        self.output_path.mkdir(parents=True, exist_ok=True)
        
        try:
            # 1. Cargar modelo y configuraci√≥n
            logger.info("üì• Cargando modelo...")
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            
            # Cargar configuraci√≥n primero para obtener arquitectura
            config = AutoConfig.from_pretrained(self.model_path)
            model_type = config.model_type
            
            # Cargar tokenizer si existe
            try:
                tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            except Exception:
                tokenizer = None
                logger.warning("Tokenizer no encontrado, continuando sin √©l")
            
            # Cargar modelo con configuraci√≥n de memoria optimizada
            logger.info(f"üñ•Ô∏è Dispositivo: {device}")
            logger.info(f"üèóÔ∏è Arquitectura: {model_type}")
            
            if device.type == "cuda":
                # Cargar en FP16 para ahorrar memoria
                model = AutoModelForCausalLM.from_pretrained(
                    self.model_path,
                    torch_dtype=torch.float16,
                    device_map="auto",
                    low_cpu_mem_usage=True,
                    trust_remote_code=True
                )
            else:
                model = AutoModelForCausalLM.from_pretrained(
                    self.model_path,
                    torch_dtype=torch.float32,
                    low_cpu_mem_usage=True,
                    trust_remote_code=True
                )
                model = model.to(device)
            
            # 2. Calcular tama√±o original
            self.stats['original_size_mb'] = sum(
                p.numel() * p.element_size() for p in model.parameters()
            ) / (1024 * 1024)
            logger.info(f"üìä Tama√±o original: {self.stats['original_size_mb']:.1f} MB")
            
            # 3. Aplicar compresi√≥n capa por capa
            logger.info("\nüîß Aplicando compresi√≥n...")
            
            # Obtener todas las capas con nombres
            named_modules = list(model.named_modules())
            total_layers = len(named_modules)
            
            with tqdm(total=total_layers, desc="Comprimiendo capas") as pbar:
                for layer_index, (name, module) in enumerate(named_modules):
                    # Determinar tipo de capa
                    layer_type = self._get_layer_type(name, module)
                    
                    # Saltar si no es una capa comprimible
                    if not self._is_compressible_layer(module):
                        pbar.update(1)
                        continue
                    
                    # Calcular posici√≥n relativa
                    relative_position = layer_index / total_layers if total_layers > 0 else 0
                    
                    # Obtener configuraci√≥n para esta capa
                    layer_config = self._get_layer_config(
                        name, layer_type, relative_position, 
                        layer_index, total_layers
                    )
                    
                    # Aplicar m√©todos de compresi√≥n
                    for method_config in layer_config['methods']:
                        method_name = method_config['name']
                        strength = method_config['strength']
                        
                        if method_name != 'none' and strength > 0:
                            logger.debug(f"  Aplicando {method_name} ({strength*100:.0f}%) a {name}")
                            
                            # Aplicar m√©todo usando el motor
                            compressed_module = self.engine.apply_method(
                                module, method_name, strength, layer_config
                            )
                            
                            # Reemplazar m√≥dulo si fue modificado
                            if compressed_module is not module:
                                self._replace_module(model, name, compressed_module)
                            
                            self.stats['methods_used'].add(method_name)
                    
                    # Actualizar estad√≠sticas
                    if layer_config['total_compression_ratio'] > 0:
                        self.stats['layers_compressed'] += 1
                    
                    pbar.update(1)
            
            # 4. Optimizaciones post-compresi√≥n
            logger.info("\n‚ö° Aplicando optimizaciones finales...")
            
            # Limpiar buffers no usados
            self._cleanup_model(model)
            
            # 5. Guardar modelo comprimido
            logger.info("\nüíæ Guardando modelo comprimido...")
            model.save_pretrained(self.output_path)
            if tokenizer is not None:
                tokenizer.save_pretrained(self.output_path)
            
            # 6. Copiar archivos adicionales
            self._copy_additional_files()
            
            # 7. Guardar estad√≠sticas y configuraci√≥n
            self._save_compression_info()
            
            # Calcular tiempo total
            self.stats['compression_time_seconds'] = (datetime.now() - start_time).total_seconds()
            
            # 8. Mostrar resumen
            self._print_summary()
            
            # Limpiar memoria
            del model
            gc.collect()
            if device.type == "cuda":
                torch.cuda.empty_cache()
            
        except Exception as e:
            logger.error(f"‚ùå Error durante la compresi√≥n: {str(e)}")
            raise
    
    def _copy_additional_files(self):
        """Copia archivos adicionales del modelo original"""
        files_to_copy = [
            'config.json',
            'generation_config.json',
            'special_tokens_map.json',
            'tokenizer_config.json',
            'tokenizer.json',
            'vocab.json',
            'merges.txt',
            'added_tokens.json',
            'preprocessor_config.json'
        ]
        
        for filename in files_to_copy:
            src = self.model_path / filename
            dst = self.output_path / filename
            
            if src.exists():
                shutil.copy2(src, dst)
                logger.debug(f"üìÑ Copiado: {filename}")
    
    def _save_compression_info(self):
        """Guarda informaci√≥n sobre la compresi√≥n aplicada"""
        # Convertir set a lista para JSON
        self.stats['methods_used'] = list(self.stats['methods_used'])
        
        info = {
            'compression_date': datetime.now().isoformat(),
            'original_model': str(self.model_path),
            'compression_config': self.compression_config,
            'statistics': self.stats,
            'notes': []
        }
        
        # Agregar nota sobre capas finales
        if self.stats['final_layers_compressed'] > 0:
            info['notes'].append(
                f"Se aplic√≥ configuraci√≥n especial a las √∫ltimas {self.stats['final_layers_compressed']} capas"
            )
        
        # Guardar
        info_path = self.output_path / "compression_metadata.json"
        with open(info_path, 'w') as f:
            json.dump(info, f, indent=2)
        
        # Tambi√©n copiar la configuraci√≥n original
        shutil.copy2(self.config_path, self.output_path / "compression_config.json")
    
    def _print_summary(self):
        """Imprime resumen de la compresi√≥n"""
        # Calcular tama√±o final
        compressed_size = sum(
            os.path.getsize(os.path.join(root, file))
            for root, _, files in os.walk(self.output_path)
            for file in files
            if file.endswith(('.bin', '.safetensors', '.pt', '.pth'))
        ) / (1024 * 1024)
        
        self.stats['compressed_size_mb'] = compressed_size
        compression_ratio = 1 - (compressed_size / self.stats['original_size_mb'])
        
        logger.info(f"\n{'='*60}")
        logger.info(f"‚úÖ COMPRESI√ìN COMPLETADA")
        logger.info(f"{'='*60}")
        logger.info(f"üìä Estad√≠sticas:")
        logger.info(f"   ‚Ä¢ Tama√±o original:     {self.stats['original_size_mb']:.1f} MB")
        logger.info(f"   ‚Ä¢ Tama√±o comprimido:   {compressed_size:.1f} MB")
        logger.info(f"   ‚Ä¢ Reducci√≥n:           {compression_ratio*100:.1f}%")
        logger.info(f"   ‚Ä¢ Factor:              {self.stats['original_size_mb']/compressed_size:.1f}x")
        logger.info(f"\nüìà Capas procesadas:")
        logger.info(f"   ‚Ä¢ Comprimidas:         {self.stats['layers_compressed']}")
        logger.info(f"   ‚Ä¢ Preservadas:         {self.stats['layers_preserved']}")
        logger.info(f"   ‚Ä¢ Capas finales:       {self.stats['final_layers_compressed']}")
        logger.info(f"\nüîß M√©todos utilizados:   {', '.join(self.stats['methods_used'])}")
        logger.info(f"‚è±Ô∏è Tiempo:               {self.stats['compression_time_seconds']:.1f} segundos")
        logger.info(f"\nüíæ Modelo guardado en:   {self.output_path}")
        logger.info(f"{'='*60}")
    
    def _get_layer_type(self, name: str, module: nn.Module) -> str:
        """Determina el tipo de una capa"""
        name_lower = name.lower()
        module_type = type(module).__name__.lower()
        
        # Embeddings
        if 'embed' in name_lower or 'embed' in module_type:
            return 'embedding'
        
        # Attention
        if any(x in name_lower for x in ['attention', 'attn', 'q_proj', 'k_proj', 'v_proj', 'o_proj']):
            return 'attention'
        
        # FFN/MLP
        if any(x in name_lower for x in ['mlp', 'ffn', 'fc1', 'fc2', 'gate_proj', 'up_proj', 'down_proj']):
            return 'ffn'
        
        # Normalization
        if any(x in name_lower for x in ['norm', 'layernorm', 'ln']):
            return 'normalization'
        
        # Output/LM Head
        if any(x in name_lower for x in ['lm_head', 'output', 'classifier']):
            return 'output'
        
        return 'other'
    
    def _is_compressible_layer(self, module: nn.Module) -> bool:
        """Determina si una capa es comprimible"""
        # Solo comprimir capas con par√°metros significativos
        if not hasattr(module, 'parameters'):
            return False
        
        num_params = sum(p.numel() for p in module.parameters())
        
        # Umbral m√≠nimo de par√°metros
        return num_params > 10000
    
    def _replace_module(self, model: nn.Module, module_name: str, new_module: nn.Module):
        """Reemplaza un m√≥dulo en el modelo"""
        parts = module_name.split('.')
        parent = model
        
        # Navegar hasta el padre
        for part in parts[:-1]:
            parent = getattr(parent, part)
        
        # Reemplazar
        setattr(parent, parts[-1], new_module)
    
    def _cleanup_model(self, model: nn.Module):
        """Limpia buffers y optimiza el modelo"""
        # Eliminar buffers no esenciales
        for name, module in model.named_modules():
            # Limpiar cach√©s de atenci√≥n si existen
            if hasattr(module, 'attention_cache'):
                delattr(module, 'attention_cache')
            
            # Compactar pesos si es posible
            for param_name, param in module.named_parameters():
                if param.grad is not None:
                    param.grad = None


def main():
    parser = argparse.ArgumentParser(
        description='Aplica compresi√≥n a un modelo seg√∫n configuraci√≥n JSON',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ejemplos:
  # Comprimir usando configuraci√≥n generada
  python apply_compression.py llama-7b
  
  # Especificar archivo de configuraci√≥n
  python apply_compression.py --config ./configs/mi_config.json
  
  # Usar sufijo personalizado
  python apply_compression.py llama-7b --suffix _optimized
        """
    )
    
    parser.add_argument(
        'model',
        type=str,
        nargs='?',
        help='Nombre del modelo a comprimir'
    )
    
    parser.add_argument(
        '--config',
        type=str,
        help='Ruta al archivo de configuraci√≥n (default: busca en compression_analysis/)'
    )
    
    parser.add_argument(
        '--models-dir',
        type=str,
        default='./models',
        help='Directorio de modelos (default: ./models)'
    )
    
    parser.add_argument(
        '--suffix',
        type=str,
        default='_compressed',
        help='Sufijo para el modelo comprimido (default: _compressed)'
    )
    
    parser.add_argument(
        '--force',
        action='store_true',
        help='Sobrescribir si el modelo comprimido ya existe'
    )
    
    args = parser.parse_args()
    
    # Determinar archivo de configuraci√≥n
    if args.config:
        config_path = args.config
    elif args.model:
        # Buscar en directorio por defecto
        config_path = f"./compression_analysis/{args.model}_compression_config.json"
    else:
        parser.error("Debes especificar un modelo o un archivo de configuraci√≥n")
    
    # Verificar que existe
    if not Path(config_path).exists():
        logger.error(f"‚ùå No se encontr√≥ archivo de configuraci√≥n: {config_path}")
        logger.error("   Primero ejecuta: python create_compression_config.py <modelo>")
        sys.exit(1)
    
    # Verificar si ya existe el modelo comprimido
    models_dir = Path(args.models_dir)
    with open(config_path, 'r') as f:
        model_name = json.load(f)['metadata']['model_name']
    
    output_path = models_dir / f"{model_name}{args.suffix}"
    if output_path.exists() and not args.force:
        logger.error(f"‚ùå El modelo comprimido ya existe: {output_path}")
        logger.error("   Usa --force para sobrescribir")
        sys.exit(1)
    
    try:
        # Crear compresor y ejecutar
        compressor = ModelCompressor(config_path, args.models_dir, args.suffix)
        compressor.compress_model()
        
        # Sugerir pr√≥ximos pasos
        logger.info("\nüìù Pr√≥ximos pasos:")
        logger.info(f"1. Verificar el modelo:")
        logger.info(f"   python verify_compression.py {model_name}")
        logger.info(f"\n2. Probar el modelo:")
        logger.info(f"   python ollama_compact_server.py --model {model_name}{args.suffix}")
        logger.info(f"\n3. Fine-tuning (si es necesario):")
        logger.info(f"   python finetune_lora.py")
        
    except Exception as e:
        logger.error(f"\n‚ùå Error: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()