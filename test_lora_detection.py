#!/usr/bin/env python3
"""
Script para testear la detecci√≥n de m√≥dulos LoRA
"""
import torch
from pathlib import Path
import json
from transformers import GPT2Config

def test_lora_detection():
    """Test simple para verificar la detecci√≥n de m√≥dulos LoRA"""
    
    model_dir = Path("models/microsoft_DialoGPT-small_compressed")
    
    if not model_dir.exists():
        print("‚ùå No se encontr√≥ el modelo comprimido")
        return
    
    print("üß™ Testeando detecci√≥n de m√≥dulos LoRA...")
    
    try:
        # Cargar configuraci√≥n
        config_path = model_dir / 'config.json'
        with open(config_path, 'r') as f:
            config_dict = json.load(f)
        
        # Convertir diccionario a objeto de configuraci√≥n
        config = GPT2Config(**config_dict)
        
        # Crear modelo desde configuraci√≥n
        from transformers import AutoModelForCausalLM
        model = AutoModelForCausalLM.from_config(config)
        
        print("‚úÖ Modelo creado desde configuraci√≥n")
        
        # Simular la detecci√≥n de m√≥dulos como en el trainer
        target_modules = []
        
        # Para modelos GPT-2, los m√≥dulos t√≠picos son:
        for name, module in model.named_modules():
            if any(keyword in name for keyword in ['attn.c_attn', 'attn.c_proj', 'mlp.c_fc', 'mlp.c_proj']):
                target_modules.append(name)
        
        print(f"üîç M√≥dulos detectados autom√°ticamente: {len(target_modules)}")
        if target_modules:
            print("  Primeros 5 m√≥dulos:")
            for i, module in enumerate(target_modules[:5]):
                print(f"    {i+1}. {module}")
        
        # Si no se encontraron, usar nombres m√°s gen√©ricos
        if not target_modules:
            target_modules = ['c_attn', 'c_proj', 'c_fc']
            print("  Usando nombres gen√©ricos")
        
        # Si a√∫n no hay m√≥dulos, usar una lista hardcodeada
        if not target_modules:
            target_modules = [
                'transformer.h.0.attn.c_attn',
                'transformer.h.0.attn.c_proj',
                'transformer.h.0.mlp.c_fc',
                'transformer.h.0.mlp.c_proj'
            ]
            print("  Usando m√≥dulos hardcodeados")
        
        # Si a√∫n no hay m√≥dulos, usar una lista m√°s completa
        if not target_modules:
            target_modules = []
            for i in range(12):  # GPT-2 small tiene 12 capas
                target_modules.extend([
                    f'transformer.h.{i}.attn.c_attn',
                    f'transformer.h.{i}.attn.c_proj',
                    f'transformer.h.{i}.mlp.c_fc',
                    f'transformer.h.{i}.mlp.c_proj'
                ])
            print(f"  Usando m√≥dulos completos: {len(target_modules)} m√≥dulos")
        
        print(f"\nüéØ M√≥dulos objetivo finales: {len(target_modules)}")
        if len(target_modules) <= 10:
            for i, module in enumerate(target_modules):
                print(f"  {i+1}. {module}")
        else:
            print("  Primeros 10 m√≥dulos:")
            for i, module in enumerate(target_modules[:10]):
                print(f"    {i+1}. {module}")
            print(f"  ... y {len(target_modules) - 10} m√°s")
        
        # Verificar que los m√≥dulos existen en el modelo
        print("\nüîç Verificando existencia de m√≥dulos:")
        existing_modules = []
        missing_modules = []
        
        for module_name in target_modules:
            try:
                # Intentar acceder al m√≥dulo
                module = model.get_submodule(module_name)
                existing_modules.append(module_name)
                print(f"  ‚úÖ {module_name}: {type(module).__name__}")
            except Exception as e:
                missing_modules.append(module_name)
                print(f"  ‚ùå {module_name}: {e}")
        
        print(f"\nüìä Resumen:")
        print(f"  ‚úÖ M√≥dulos existentes: {len(existing_modules)}")
        print(f"  ‚ùå M√≥dulos faltantes: {len(missing_modules)}")
        
        if existing_modules:
            print(f"\nüéâ ¬°√âxito! Se encontraron {len(existing_modules)} m√≥dulos v√°lidos para LoRA")
        else:
            print(f"\nüí• Error: No se encontraron m√≥dulos v√°lidos para LoRA")
        
        print("\n‚úÖ Test completado!")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_lora_detection()
