#!/usr/bin/env python3
"""
Script simple para testear el modelo comprimido
"""
import torch
from pathlib import Path
import json
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig

def test_compressed_model():
    """Test simple del modelo comprimido"""
    
    model_dir = Path("models/microsoft_DialoGPT-small_compressed")
    
    if not model_dir.exists():
        print("‚ùå No se encontr√≥ el modelo comprimido")
        return
    
    print("üß™ Testeando modelo comprimido...")
    
    try:
        # Cargar tokenizer
        tokenizer = AutoTokenizer.from_pretrained("models/microsoft_DialoGPT-small")
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print("‚úÖ Tokenizer cargado")
        
        # Cargar config
        config = AutoConfig.from_pretrained(str(model_dir))
        print("‚úÖ Config cargado")
        
        # Crear modelo
        model = AutoModelForCausalLM.from_config(config)
        print("‚úÖ Modelo creado")
        
        # Cargar par√°metros
        state_dict = {}
        param_files = list(model_dir.glob("*.pt"))
        print(f"üîç Encontrados {len(param_files)} archivos de par√°metros")
        
        # Mapeo de nombres de archivo a nombres de par√°metros
        name_mapping = {}
        for param_file in param_files:
            simple_name = param_file.stem
            
            # Mapear nombres correctamente
            if "transformer_h_" in simple_name:
                parts = simple_name.split("_")
                if len(parts) >= 4:
                    layer_num = parts[2]
                    layer_type = parts[3]
                    
                    if layer_type == "attn":
                        if "c_attn" in simple_name:
                            if "bias" in simple_name:
                                param_name = f"transformer.h.{layer_num}.attn.c_attn.bias"
                            else:
                                param_name = f"transformer.h.{layer_num}.attn.c_attn.weight"
                        elif "c_proj" in simple_name:
                            if "bias" in simple_name:
                                param_name = f"transformer.h.{layer_num}.attn.c_proj.bias"
                            else:
                                param_name = f"transformer.h.{layer_num}.attn.c_proj.weight"
                    elif layer_type == "mlp":
                        if "c_fc" in simple_name:
                            if "bias" in simple_name:
                                param_name = f"transformer.h.{layer_num}.mlp.c_fc.bias"
                            else:
                                param_name = f"transformer.h.{layer_num}.mlp.c_fc.weight"
                        elif "c_proj" in simple_name:
                            if "bias" in simple_name:
                                param_name = f"transformer.h.{layer_num}.mlp.c_proj.bias"
                            else:
                                param_name = f"transformer.h.{layer_num}.mlp.c_proj.weight"
                    elif layer_type == "ln":
                        if "ln_1" in simple_name:
                            if "bias" in simple_name:
                                param_name = f"transformer.h.{layer_num}.ln_1.bias"
                            else:
                                param_name = f"transformer.h.{layer_num}.ln_1.weight"
                        elif "ln_2" in simple_name:
                            if "bias" in simple_name:
                                param_name = f"transformer.h.{layer_num}.ln_2.bias"
                            else:
                                param_name = f"transformer.h.{layer_num}.ln_2.weight"
                    else:
                        param_name = simple_name
                else:
                    param_name = simple_name
            elif "transformer_ln_f" in simple_name:
                if "bias" in simple_name:
                    param_name = "transformer.ln_f.bias"
                else:
                    param_name = "transformer.ln_f.weight"
            elif "transformer_wte" in simple_name:
                param_name = "transformer.wte.weight"
            elif "transformer_wpe" in simple_name:
                param_name = "transformer.wpe.weight"
            else:
                param_name = simple_name
            
            name_mapping[simple_name] = param_name
        
        # Cargar par√°metros
        for param_file in param_files:
            try:
                simple_name = param_file.stem
                param_name = name_mapping.get(simple_name, simple_name)
                
                param_tensor = torch.load(param_file, map_location="cpu")
                
                # Verificar que el tensor tiene la forma correcta
                if param_name in model.state_dict():
                    expected_shape = model.state_dict()[param_name].shape
                    if param_tensor.shape == expected_shape:
                        state_dict[param_name] = param_tensor
                        print(f"‚úÖ {simple_name} -> {param_name} ({param_tensor.shape})")
                    else:
                        print(f"‚ö†Ô∏è Forma incorrecta: {simple_name} -> {param_name} (esperado: {expected_shape}, actual: {param_tensor.shape})")
                else:
                    print(f"‚ö†Ô∏è Par√°metro no encontrado: {param_name}")
                
            except Exception as e:
                print(f"‚ùå Error cargando {param_file.name}: {e}")
                continue
        
        print(f"\nüìä Par√°metros cargados: {len(state_dict)}")
        
        # Cargar estado del modelo
        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
        print(f"‚ö†Ô∏è Claves faltantes: {len(missing_keys)}")
        print(f"‚ö†Ô∏è Claves inesperadas: {len(unexpected_keys)}")
        
        if len(missing_keys) < 10:  # Si faltan pocas claves, intentar generar
            print("\nüéØ Intentando generaci√≥n de texto...")
            
            model.eval()
            prompt = "Hello"
            inputs = tokenizer(prompt, return_tensors="pt")
            
            with torch.no_grad():
                outputs = model.generate(
                    inputs['input_ids'],
                    max_length=len(inputs['input_ids'][0]) + 20,
                    num_return_sequences=1,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"‚úÖ Texto generado: {generated_text}")
            
        else:
            print("‚ùå Demasiadas claves faltantes para generar texto")
            print("üîç Primeras 10 claves faltantes:")
            for i, key in enumerate(missing_keys[:10]):
                print(f"   {i+1}. {key}")
        
        print("\n‚úÖ Test completado!")
        
    except Exception as e:
        print(f"‚ùå Error durante el test: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_compressed_model()
